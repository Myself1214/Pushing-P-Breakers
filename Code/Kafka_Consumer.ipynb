{"cells":[{"cell_type":"code","source":["# Define Error Callbacks\ndef error_cb(err):\n    \"\"\" The error callback is used for generic client errors. These\n        errors are generally to be considered informational as the client will\n        automatically try to recover from all errors, and no extra action\n        is typically required by the application.\n        For this example however, we terminate the application if the client\n        is unable to connect to any broker (_ALL_BROKERS_DOWN) and on\n        authentication errors (_AUTHENTICATION). \"\"\"\n\n    print(\"Client error: {}\".format(err))\n    if err.code() == KafkaError._ALL_BROKERS_DOWN or \\\n       err.code() == KafkaError._AUTHENTICATION:\n        # Any exception raised from this callback will be re-raised from the\n        # triggering flush() or poll() call.\n        raise KafkaException(err)\n\n\ndef acked(err, msg):\n    \"\"\" \n        Error callback is used for generic issues for producer errors. \n        \n        Parameters:\n            err (err): Error flag.\n            msg (str): Error message that was part of the callback.\n    \"\"\"\n    if err is not None:\n        print(\"Failed to deliver message: %s: %s\" % (str(msg), str(err)))\n    else:\n        print(\"Message produced: %s\" % (str(msg)))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"0c4c7243-231a-4269-a23b-9867aafafa12"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["# Connection String\nfrom confluent_kafka import Consumer\nfrom time import sleep\nimport uuid\nfrom confluent_kafka import Consumer, KafkaError, KafkaException\nimport json\nfrom confluent_kafka.admin import AdminClient, NewTopic\n\n##### CHANGE YOUR CONFLUENTTOPICNAME TO YOUR TOPIC ######### \n\n#KAFKA variables, Move to the OS variables or configuration\n# This will work in local Jupiter Notebook, but in a databrick, hiding config.py is tougher. \nconfluentClusterName = \"stage3talent\"\nconfluentBootstrapServers = \"pkc-ldvmy.centralus.azure.confluent.cloud:9092\"\nconfluentTopicName = \"pushing-p\"  ## MAKE SURE YOU CHANGE THIS\nschemaRegistryUrl = \"https://psrc-gq7pv.westus2.azure.confluent.cloud\"\nconfluentApiKey = \"YHMHG7E54LJA55XZ\"\nconfluentSecret = \"/XYn+w3gHGMqpe9l0TWvA9FznMYNln2STI+dytyPqtZ9QktH0TbGXUqepEsJ/nR0\"\nconfluentRegistryApiKey = \"YHMHG7E54LJA55XZ\"\nconfluentRegistrySecret = \"/XYn+w3gHGMqpe9l0TWvA9FznMYNln2STI+dytyPqtZ9QktH0TbGXUqepEsJ/nR0\"\n\n\n#Kakfa Class Setup.\nc = Consumer({\n    'bootstrap.servers': confluentBootstrapServers,\n    'sasl.mechanism': 'PLAIN',\n    'security.protocol': 'SASL_SSL',\n    'sasl.username': confluentApiKey,\n    'sasl.password': confluentSecret,\n    'group.id': str(uuid.uuid1()),  # this will create a new consumer group on each invocation.\n    'auto.offset.reset': 'earliest',\n    'error_cb': error_cb,\n})\n\nc.subscribe(['pushing-p'])"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b95117c3-dc23-4f96-b440-cce9a59e9786"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["# List of Counties in New Jersey (ordered Alphabetically)\ncounty_list = ['Atlantic County',\n 'Bergen County',\n 'Burlington County',\n 'Camden County',\n 'Cape May County',\n 'Cumberland County',\n 'Essex County',\n 'Gloucester County',\n 'Hudson County',\n 'Hunterdon County','Mercer County', 'Middlesex County', 'Monmouth County', 'Morris County', 'Ocean County', 'Passaic County', 'Salem County', 'Somerset County', 'Sussex County', 'Union County', 'Warren County']"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"785b163a-2ff5-4544-8a48-7578ea8fbc72"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["admin_database = \"master\"\nadmin_user = \"gen10dbadmin\"\nadmin_password  = \"vbtwrEmgSG12mabBr9ReZkcPrrDbTR-Y\"\nadmin_server = \"gen10-data-fundamentals-22-02-sql-server.database.windows.net\"\n\ndatabase = \"Pushing-P-DB\"\ntable = \"dbo.Live_Feed\"\nuser = \"pushing_p\"\npassword  = \"t3stP@ssword\"\nserver = \"gen10-data-fundamentals-22-02-sql-server.database.windows.net\""],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ab73bd8e-4ec7-41af-bf99-82a827dbe51e"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["# Consume messages from Topic in chunks of 10000\n\naString = {}\n\nkafkaListDictionaries = []\n\nj = 0\n\nfor i in range(1000):\n    \n    try:\n        \n        msg = c.poll(timeout=15)\n        \n        if msg is None:  # When we run out of messages add whatever messages remain to SQL Database\n          \n            # Create DataFrame from Messages\n            county_df = spark.createDataFrame(kafkaListDictionaries)\n\n            # Write PySpark DataFrame to SQL Database \n            county_df.write.format('jdbc').option(\"url\", f\"jdbc:sqlserver://{server}:1433;databaseName={database};\") \\\n                .mode(\"overwrite\") \\\n                .option(\"dbtable\", table) \\\n                .option(\"user\", user) \\\n                .option(\"password\", password) \\\n                .option(\"driver\", \"com.microsoft.sqlserver.jdbc.SQLServerDriver\") \\\n                .save()\n            \n            break\n            \n        elif msg.error():\n            print(\"Consumer error: {}\".format(msg.error()))\n            break\n            \n        else:\n            \n            aString=json.loads('{}'.format(msg.value().decode('utf-8')))\n            aString['Timestamp'] = msg.timestamp()[1]\n            \n            # Get Current County\n            current_county = county_list[j]\n            \n            # If the county changed, upload the current messages to SQL\n            if aString['County'] != current_county:\n                \n                # Create DataFrame from Messages\n                county_df = spark.createDataFrame(kafkaListDictionaries)\n                \n                # Write PySpark DataFrame to SQL Database \n                county_df.write.format('jdbc').option(\"url\", f\"jdbc:sqlserver://{server}:1433;databaseName={database};\") \\\n                    .mode(\"overwrite\") \\\n                    .option(\"dbtable\", table) \\\n                    .option(\"user\", user) \\\n                    .option(\"password\", password) \\\n                    .option(\"driver\", \"com.microsoft.sqlserver.jdbc.SQLServerDriver\") \\\n                    .save()\n                \n                # Reset List of messages and move to next county\n                kafkaListDictionaries = []\n                j = j + 1\n                \n                sleep(30)\n                \n            kafkaListDictionaries.append(aString)\n            c.commit(asynchronous=False)\n            \n    except Exception as e:\n        print(e)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"76f6ccfd-62fb-4410-b853-4b7251b547c1"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">KafkaError{code=_NO_OFFSET,val=-168,str=&#34;Commit failed: Local: No offset stored&#34;}\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">KafkaError{code=_NO_OFFSET,val=-168,str=&#34;Commit failed: Local: No offset stored&#34;}\n</div>"]}}],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"Capstone Project Consumer","dashboards":[],"notebookMetadata":{"pythonIndentUnit":4},"language":"python","widgets":{},"notebookOrigID":1967226895816147}},"nbformat":4,"nbformat_minor":0}
